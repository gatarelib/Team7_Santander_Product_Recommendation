{
  "paragraphs": [
    {
      "title": "Reading train dataset",
      "text": "val train_data \u003d sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Users/vaishalilambe/Downloads/Team_7_Santander_Product_Recommendation/data-cleaning-app/src/main/resources/new_train.csv\")\n\ntrain_data.toDF().registerTempTable(\"sbank\")",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "train_data: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842229_-1828951939",
      "id": "20180402-160438_1178221967",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "1. Handling data type of Age [Cust_Age] column, convert from String to Int",
      "text": "import spark.implicits._\nval train_data_new \u003d train_data.withColumn(\"Cust_Age\", train_data(\"Cust_Age\").cast(IntegerType))\ntrain_data_new.toDF().registerTempTable(\"sbank1\")",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import spark.implicits._\ntrain_data_new: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842229_-1828951939",
      "id": "20180402-161051_266338280",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "2. Handling NULL values of fecha_alta [First_Holder_Date], replacing those with mean",
      "text": "import org.apache.spark.sql.functions._\nimport spark.implicits._\n\n// calculate mean\nval median_date \u003d train_data_new.groupBy(\"First_Holder_Date\").agg(median(\"First_Holder_Date\") as \"median\")\n\n// use join, select and \"nanvl\" function to replace Null  with the mean values:\nval result \u003d train_data_new\n  .join(median_date, \"First_Holder_Date\")\n  .select($\"First_Holder_Date\", coalesce($\"First_Holder_Date\", $\"median\")).show()",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions._\nimport spark.implicits._\n\u003cconsole\u003e:75: error: not found: value median\n       val median_date \u003d train_data_new.groupBy(\"First_Holder_Date\").agg(median(\"First_Holder_Date\") as \"median\")\n                                                                         ^\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842230_-1827797692",
      "id": "20180402-163300_1833936216",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "3. Handling NULL values of nomprov [Province_Name], replacing those with \"Unknown\"",
      "text": "import org.apache.spark.sql.functions.udf\nval Province_Name:(String\u003d\u003eString)\u003d(Province_Name:String)\u003d\u003e{ \n    Province_Name match {case null \u003d\u003e \"UNKNOWN\" case a \u003d\u003e a}}\n                                       \n val Province_Name_udf\u003dudf(Province_Name)\n  val train_data_new2 \u003d train_data_new.withColumn(\"Province_Name\", Province_Name_udf(col(\"Province_Name\")))\n\ntrain_data_new2.toDF.registerTempTable(\"sbank3\")",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.udf\nProvince_Name: String \u003d\u003e String \u003d \u003cfunction1\u003e\nProvince_Name_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\ntrain_data_new2: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842231_-1828182441",
      "id": "20180402-164514_1354830716",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.sql\nselect Province_Name, count(Province_Name) as Count_Province_Name \nfrom sbank3 \nwhere Province_Name \u003d \"UNKNOWN\"\ngroup by Province_Name\n",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 84.0,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "Province_Name\tCount_Province_Name\nUNKNOWN\t93591\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842231_-1828182441",
      "id": "20180402-165441_2001756054",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "4. Handling NULL values of sexo [Cust_Gender], replacing those with ratio of gender",
      "text": "val Cust_Gender_Clean:(String \u003d\u003e String) \u003d (Cust_Gender:String) \u003d\u003e {val r \u003d new scala.util.Random;\n                                                Cust_Gender.replace(\" \",\"\") match {case \"\" \u003d\u003e if(r.nextFloat\u003e0.54) \"V\" else \"H\" case a \u003d\u003e a}}\nval Cust_Gender_udf \u003d udf(Cust_Gender_Clean)\n\nval train_data_new3 \u003d train_data_new2.withColumn(\"Cust_Gender\", Cust_Gender_udf(col(\"Cust_Gender\")))\n\ntrain_data_new3.toDF().registerTempTable(\"sbank4\")\n",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Cust_Gender_Clean: String \u003d\u003e String \u003d \u003cfunction1\u003e\nCust_Gender_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\ntrain_data_new3: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842232_-1830106186",
      "id": "20180402-170827_803757282",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.sql\nselect Cust_Gender, count(Cust_Gender) as Count_Gender \nfrom sbank4 \ngroup by Cust_Gender ",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "sql"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "java.lang.NullPointerException\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842232_-1830106186",
      "id": "20180402-170435_281489052",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "5.  Handling NULL values of indfall [Deceased_Index], replacing those with ratio of index",
      "text": "val Deceased_Index_Clean:(String \u003d\u003e String) \u003d (Deceased_Index:String) \u003d\u003e {val r \u003d new scala.util.Random;\n                                                Deceased_Index.replace(\" \",\"\") match {case \"\" \u003d\u003e if(r.nextFloat\u003e0.54) \"S\" else \"N\" case a \u003d\u003e a}}\nval Deceased_Index_udf \u003d udf(Deceased_Index_Clean)\n\nval train_data_new4 \u003d train_data_new3.withColumn(\"Deceased_Index\", Cust_Gender_udf(col(\"Deceased_Index\")))\n\ntrain_data_new4.toDF().registerTempTable(\"sbank5\")\n",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Deceased_Index_Clean: String \u003d\u003e String \u003d \u003cfunction1\u003e\nDeceased_Index_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\ntrain_data_new4: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842232_-1830106186",
      "id": "20180402-171628_1811724511",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "6. Handling NULL values of indresi [Residence_Index], replacing those with ratio of index",
      "text": "val Residence_Index_Clean:(String \u003d\u003e String) \u003d (Residence_Index:String) \u003d\u003e {val r \u003d new scala.util.Random;\n                                                Residence_Index.replace(\" \",\"\") match {case \"\" \u003d\u003e if(r.nextFloat\u003e0.54) \"S\" else \"N\" case a \u003d\u003e a}}\nval Residence_Index_udf \u003d udf(Residence_Index_Clean)\n\nval train_data_new5 \u003d train_data_new4.withColumn(\"Residence_Index\", Cust_Gender_udf(col(\"Residence_Index\")))\n\ntrain_data_new5.toDF().registerTempTable(\"sbank6\")\n",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Residence_Index_Clean: String \u003d\u003e String \u003d \u003cfunction1\u003e\nResidence_Index_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\ntrain_data_new5: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842233_-1830490934",
      "id": "20180402-172318_856473557",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "7. Handling NULL values of ind_empleado [Emp_Index], replacing those with ratio of index",
      "text": "val Emp_Index_Clean:(String \u003d\u003e String) \u003d (Emp_Index:String) \u003d\u003e {val r \u003d new scala.util.Random;\n                                                Emp_Index.replace(\" \",\"\") match {case \"\" \u003d\u003e if(r.nextFloat\u003e0.54) \"S\" else \"N\" case a \u003d\u003e a}}\nval Emp_Index_udf \u003d udf(Emp_Index_Clean)\n\nval train_data_new6 \u003d train_data_new5.withColumn(\"Emp_Index\", Cust_Gender_udf(col(\"Emp_Index\")))\n\ntrain_data_new6.toDF().registerTempTable(\"sbank7\")",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Emp_Index_Clean: String \u003d\u003e String \u003d \u003cfunction1\u003e\nEmp_Index_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\ntrain_data_new6: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842233_-1830490934",
      "id": "20180402-173252_1190000956",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "8.  Handling NULL values of indrel_1mes [Cust_Type], replacing those Unknown",
      "text": "val Cust_Type_Clean:(String \u003d\u003e String) \u003d(Cust_Type:String) \u003d\u003e {Cust_Type match {case null \u003d\u003e \"UNKNOWN\" case \"P\" \u003d\u003e \"P\" case a \u003d\u003e a.toDouble.toInt.toString}}\nval Cust_Type_udf \u003d udf(Cust_Type_Clean)\n\nval train_data_new7 \u003d train_data_new6.withColumn(\"Cust_Type\", Cust_Type_udf(col(\"Cust_Type\")))\n\ntrain_data_new7.toDF().registerTempTable(\"sbank8\")",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Cust_Type_Clean: String \u003d\u003e String \u003d \u003cfunction1\u003e\nCust_Type_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\ntrain_data_new7: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842233_-1830490934",
      "id": "20180402-173407_521822728",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "9. Handling NULL values of tiprel_1mes [Cust_Relation_type], replacing those with ratio of type",
      "text": "val Cust_Relation_type_Clean:(String \u003d\u003e String) \u003d (Cust_Relation_type:String) \u003d\u003e {val r \u003d new scala.util.Random;\n                                                Cust_Relation_type.replace(\" \",\"\") match {case \"\" \u003d\u003e if(r.nextFloat\u003e0.54) \"A\" else \"I\" case a \u003d\u003e a}}\nval Cust_Relation_type_udf \u003d udf(Cust_Relation_type_Clean)\n\nval train_data_new8 \u003d train_data_new7.withColumn(\"Cust_Type\", Cust_Relation_type_udf(col(\"Cust_Relation_type\")))\n\ntrain_data_new8.toDF().registerTempTable(\"sbank9\")",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Cust_Relation_type_Clean: String \u003d\u003e String \u003d \u003cfunction1\u003e\nCust_Relation_type_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\ntrain_data_new8: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842233_-1830490934",
      "id": "20180402-173453_75142870",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "10. Handling NULL values of indext [Foreigner_Index], replacing those with ratio of index",
      "text": "val Foreigner_Index_Clean:(String \u003d\u003e String) \u003d (Foreigner_Index:String) \u003d\u003e {val r \u003d new scala.util.Random;\n                                                Foreigner_Index.replace(\" \",\"\") match {case \"\" \u003d\u003e if(r.nextFloat\u003e0.46) \"S\" else \"N\" case a \u003d\u003e a}}\nval Foreigner_Index_udf \u003d udf(Emp_Index_Clean)\n\nval train_data_new9 \u003d train_data_new8.withColumn(\"Foreigner_Index\", Foreigner_Index_udf(col(\"Foreigner_Index\")))\n\ntrain_data_new9.toDF().registerTempTable(\"sbank10\")",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Foreigner_Index_Clean: String \u003d\u003e String \u003d \u003cfunction1\u003e\nForeigner_Index_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\ntrain_data_new9: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842234_-1829336688",
      "id": "20180402-173606_887947438",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "11. Handling NULL values of canal_entrada [Channel], replacing those with Unknown",
      "text": "val Channel_Clean:(String \u003d\u003e String) \u003d(Channel:String) \u003d\u003e {Channel match {case null \u003d\u003e \"UNKNOWN\" case a \u003d\u003e a}}\nval Channel_udf \u003d udf(Channel_Clean)\n\nval train_data_new10 \u003d train_data_new9.withColumn(\"Channel\", Channel_udf(col(\"Channel\")))\n\ntrain_data_new10.toDF().registerTempTable(\"sbank11\")",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Channel_Clean: String \u003d\u003e String \u003d \u003cfunction1\u003e\nChannel_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\ntrain_data_new10: org.apache.spark.sql.DataFrame \u003d [Partitioned_Date: timestamp, Cust_Code: double ... 46 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842234_-1829336688",
      "id": "20180402-173647_395253741",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "12. Handling NULL values of renta [Gross_Income], replacing those with average value of income",
      "text": "import org.apache.spark.sql.functions._\nimport spark.implicits._\n\n// calculate mean per category:\nval meanIncome \u003d train_data_new10.groupBy(\"Gross_Income\").agg(mean(\"Gross_Income\") as \"mean\")\n\n// use join, select and \"nanvl\" function to replace NaNs with the mean values:\nval result \u003d train_data_new10\n  .join(meanIncome, \"Gross_Income\")\n  .select($\"Gross_Income\", $\"id\", coalesce($\"Gross_Income\", $\"mean\")).show()",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions._\nimport spark.implicits._\nmeanIncome: org.apache.spark.sql.DataFrame \u003d [Gross_Income: double, mean: double]\norg.apache.spark.sql.AnalysisException: cannot resolve \u0027`id`\u0027 given input columns: [Mortgage, Payroll, Saving_Acc, Credit_Card, Cust_Residence, Province_Name, Direct_Debit, Mas_Acc, Cust_Index_Primary, Current_Acc, Nom_Pensions, Residence_Index, First_Holder_Date, Foreigner_Index, Partitioned_Date, Cust_Relation_Type, Cust_Gender, Junior_Acc, Taxes, Activity_Index, Deceased_Index, Addres_Type, Home_Acc, Pensions, Loans, Medium_Term_Deposits, Derivada_Acc, Cust_Identification, Province_Code, Cust_Type, Particular_Plus_Acc, Funds, Short_Term_Deposit, Guarantees, Spouse_Index, Emp_Index, New_Cust_Index, Particular_Acc, Payroll_Acc, e_Acc, Gross_Income, Cust_Code, Cust_Age, Channel, Cust_Seniority, Securities, Last_Date_Primary_Cust, Long_Term_Deposits, mean];;\n\u0027Project [Gross_Income#411, \u0027id, coalesce(Gross_Income#411, mean#2619) AS coalesce(Gross_Income, mean)#2721]\n+- Project [Gross_Income#411, Partitioned_Date#389, Cust_Code#390, Emp_Index#1866, Cust_Residence#392, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, Cust_Type#2066, Cust_Relation_Type#401, Residence_Index#1766, Foreigner_Index#2166, Spouse_Index#404, Channel#2316, Deceased_Index#1666, Addres_Type#407, Province_Code#408, Province_Name#1091, Activity_Index#410, Cust_Identification#412, ... 25 more fields]\n   +- Join Inner, (Gross_Income#411 \u003d Gross_Income#2645)\n      :- Project [Partitioned_Date#389, Cust_Code#390, Emp_Index#1866, Cust_Residence#392, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, Cust_Type#2066, Cust_Relation_Type#401, Residence_Index#1766, Foreigner_Index#2166, Spouse_Index#404, UDF(Channel#405) AS Channel#2316, Deceased_Index#1666, Addres_Type#407, Province_Code#408, Province_Name#1091, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :  +- Project [Partitioned_Date#389, Cust_Code#390, Emp_Index#1866, Cust_Residence#392, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, Cust_Type#2066, Cust_Relation_Type#401, Residence_Index#1766, UDF(Foreigner_Index#403) AS Foreigner_Index#2166, Spouse_Index#404, Channel#405, Deceased_Index#1666, Addres_Type#407, Province_Code#408, Province_Name#1091, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :     +- Project [Partitioned_Date#389, Cust_Code#390, Emp_Index#1866, Cust_Residence#392, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, UDF(Cust_Relation_type#401) AS Cust_Type#2066, Cust_Relation_Type#401, Residence_Index#1766, Foreigner_Index#403, Spouse_Index#404, Channel#405, Deceased_Index#1666, Addres_Type#407, Province_Code#408, Province_Name#1091, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :        +- Project [Partitioned_Date#389, Cust_Code#390, Emp_Index#1866, Cust_Residence#392, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, UDF(Cust_Type#400) AS Cust_Type#1966, Cust_Relation_Type#401, Residence_Index#1766, Foreigner_Index#403, Spouse_Index#404, Channel#405, Deceased_Index#1666, Addres_Type#407, Province_Code#408, Province_Name#1091, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :           +- Project [Partitioned_Date#389, Cust_Code#390, UDF(Emp_Index#391) AS Emp_Index#1866, Cust_Residence#392, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, Cust_Type#400, Cust_Relation_Type#401, Residence_Index#1766, Foreigner_Index#403, Spouse_Index#404, Channel#405, Deceased_Index#1666, Addres_Type#407, Province_Code#408, Province_Name#1091, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :              +- Project [Partitioned_Date#389, Cust_Code#390, Emp_Index#391, Cust_Residence#392, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, Cust_Type#400, Cust_Relation_Type#401, UDF(Residence_Index#402) AS Residence_Index#1766, Foreigner_Index#403, Spouse_Index#404, Channel#405, Deceased_Index#1666, Addres_Type#407, Province_Code#408, Province_Name#1091, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :                 +- Project [Partitioned_Date#389, Cust_Code#390, Emp_Index#391, Cust_Residence#392, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, Cust_Type#400, Cust_Relation_Type#401, Residence_Index#402, Foreigner_Index#403, Spouse_Index#404, Channel#405, UDF(Deceased_Index#406) AS Deceased_Index#1666, Addres_Type#407, Province_Code#408, Province_Name#1091, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :                    +- Project [Partitioned_Date#389, Cust_Code#390, Emp_Index#391, Cust_Residence#392, UDF(Cust_Gender#393) AS Cust_Gender#1358, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, Cust_Type#400, Cust_Relation_Type#401, Residence_Index#402, Foreigner_Index#403, Spouse_Index#404, Channel#405, Deceased_Index#406, Addres_Type#407, Province_Code#408, Province_Name#1091, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :                       +- Project [Partitioned_Date#389, Cust_Code#390, Emp_Index#391, Cust_Residence#392, Cust_Gender#393, Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, Cust_Type#400, Cust_Relation_Type#401, Residence_Index#402, Foreigner_Index#403, Spouse_Index#404, Channel#405, Deceased_Index#406, Addres_Type#407, Province_Code#408, UDF(Province_Name#409) AS Province_Name#1091, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :                          +- Project [Partitioned_Date#389, Cust_Code#390, Emp_Index#391, Cust_Residence#392, Cust_Gender#393, cast(cast(Cust_Age#394 as decimal(20,0)) as int) AS Cust_Age#786, First_Holder_Date#395, New_Cust_Index#396, Cust_Seniority#397, Cust_Index_Primary#398, Last_Date_Primary_Cust#399, Cust_Type#400, Cust_Relation_Type#401, Residence_Index#402, Foreigner_Index#403, Spouse_Index#404, Channel#405, Deceased_Index#406, Addres_Type#407, Province_Code#408, Province_Name#409, Activity_Index#410, Gross_Income#411, Cust_Identification#412, ... 24 more fields]\n      :                             +- Relation[Partitioned_Date#389,Cust_Code#390,Emp_Index#391,Cust_Residence#392,Cust_Gender#393,Cust_Age#394,First_Holder_Date#395,New_Cust_Index#396,Cust_Seniority#397,Cust_Index_Primary#398,Last_Date_Primary_Cust#399,Cust_Type#400,Cust_Relation_Type#401,Residence_Index#402,Foreigner_Index#403,Spouse_Index#404,Channel#405,Deceased_Index#406,Addres_Type#407,Province_Code#408,Province_Name#409,Activity_Index#410,Gross_Income#411,Cust_Identification#412,... 24 more fields] csv\n      +- Aggregate [Gross_Income#2645], [Gross_Income#2645, avg(Gross_Income#2645) AS mean#2619]\n         +- Project [Partitioned_Date#2623, Cust_Code#2624, Emp_Index#1866, Cust_Residence#2626, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, Cust_Type#2066, Cust_Relation_Type#2635, Residence_Index#1766, Foreigner_Index#2166, Spouse_Index#2638, UDF(Channel#2639) AS Channel#2316, Deceased_Index#1666, Addres_Type#2641, Province_Code#2642, Province_Name#1091, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n            +- Project [Partitioned_Date#2623, Cust_Code#2624, Emp_Index#1866, Cust_Residence#2626, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, Cust_Type#2066, Cust_Relation_Type#2635, Residence_Index#1766, UDF(Foreigner_Index#2637) AS Foreigner_Index#2166, Spouse_Index#2638, Channel#2639, Deceased_Index#1666, Addres_Type#2641, Province_Code#2642, Province_Name#1091, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n               +- Project [Partitioned_Date#2623, Cust_Code#2624, Emp_Index#1866, Cust_Residence#2626, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, UDF(Cust_Relation_Type#2635) AS Cust_Type#2066, Cust_Relation_Type#2635, Residence_Index#1766, Foreigner_Index#2637, Spouse_Index#2638, Channel#2639, Deceased_Index#1666, Addres_Type#2641, Province_Code#2642, Province_Name#1091, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n                  +- Project [Partitioned_Date#2623, Cust_Code#2624, Emp_Index#1866, Cust_Residence#2626, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, UDF(Cust_Type#2634) AS Cust_Type#1966, Cust_Relation_Type#2635, Residence_Index#1766, Foreigner_Index#2637, Spouse_Index#2638, Channel#2639, Deceased_Index#1666, Addres_Type#2641, Province_Code#2642, Province_Name#1091, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n                     +- Project [Partitioned_Date#2623, Cust_Code#2624, UDF(Emp_Index#2625) AS Emp_Index#1866, Cust_Residence#2626, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, Cust_Type#2634, Cust_Relation_Type#2635, Residence_Index#1766, Foreigner_Index#2637, Spouse_Index#2638, Channel#2639, Deceased_Index#1666, Addres_Type#2641, Province_Code#2642, Province_Name#1091, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n                        +- Project [Partitioned_Date#2623, Cust_Code#2624, Emp_Index#2625, Cust_Residence#2626, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, Cust_Type#2634, Cust_Relation_Type#2635, UDF(Residence_Index#2636) AS Residence_Index#1766, Foreigner_Index#2637, Spouse_Index#2638, Channel#2639, Deceased_Index#1666, Addres_Type#2641, Province_Code#2642, Province_Name#1091, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n                           +- Project [Partitioned_Date#2623, Cust_Code#2624, Emp_Index#2625, Cust_Residence#2626, Cust_Gender#1358, Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, Cust_Type#2634, Cust_Relation_Type#2635, Residence_Index#2636, Foreigner_Index#2637, Spouse_Index#2638, Channel#2639, UDF(Deceased_Index#2640) AS Deceased_Index#1666, Addres_Type#2641, Province_Code#2642, Province_Name#1091, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n                              +- Project [Partitioned_Date#2623, Cust_Code#2624, Emp_Index#2625, Cust_Residence#2626, UDF(Cust_Gender#2627) AS Cust_Gender#1358, Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, Cust_Type#2634, Cust_Relation_Type#2635, Residence_Index#2636, Foreigner_Index#2637, Spouse_Index#2638, Channel#2639, Deceased_Index#2640, Addres_Type#2641, Province_Code#2642, Province_Name#1091, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n                                 +- Project [Partitioned_Date#2623, Cust_Code#2624, Emp_Index#2625, Cust_Residence#2626, Cust_Gender#2627, Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, Cust_Type#2634, Cust_Relation_Type#2635, Residence_Index#2636, Foreigner_Index#2637, Spouse_Index#2638, Channel#2639, Deceased_Index#2640, Addres_Type#2641, Province_Code#2642, UDF(Province_Name#2643) AS Province_Name#1091, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n                                    +- Project [Partitioned_Date#2623, Cust_Code#2624, Emp_Index#2625, Cust_Residence#2626, Cust_Gender#2627, cast(cast(Cust_Age#2628 as decimal(20,0)) as int) AS Cust_Age#786, First_Holder_Date#2629, New_Cust_Index#2630, Cust_Seniority#2631, Cust_Index_Primary#2632, Last_Date_Primary_Cust#2633, Cust_Type#2634, Cust_Relation_Type#2635, Residence_Index#2636, Foreigner_Index#2637, Spouse_Index#2638, Channel#2639, Deceased_Index#2640, Addres_Type#2641, Province_Code#2642, Province_Name#2643, Activity_Index#2644, Gross_Income#2645, Cust_Identification#2646, ... 24 more fields]\n                                       +- Relation[Partitioned_Date#2623,Cust_Code#2624,Emp_Index#2625,Cust_Residence#2626,Cust_Gender#2627,Cust_Age#2628,First_Holder_Date#2629,New_Cust_Index#2630,Cust_Seniority#2631,Cust_Index_Primary#2632,Last_Date_Primary_Cust#2633,Cust_Type#2634,Cust_Relation_Type#2635,Residence_Index#2636,Foreigner_Index#2637,Spouse_Index#2638,Channel#2639,Deceased_Index#2640,Addres_Type#2641,Province_Code#2642,Province_Name#2643,Activity_Index#2644,Gross_Income#2645,Cust_Identification#2646,... 24 more fields] csv\n\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:282)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:292)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:296)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:296)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$7.apply(QueryPlan.scala:301)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:301)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2822)\n  at org.apache.spark.sql.Dataset.select(Dataset.scala:1121)\n  ... 90 elided\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842234_-1829336688",
      "id": "20180402-173727_463987271",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "13. Handling NULL values of segmento [Cust_Identification], replacing those with unknown",
      "text": "val Cust_Identification:(String\u003d\u003eString)\u003d(Cust_Identification:String)\u003d\u003e{ \n    Cust_Identification match {case null \u003d\u003e \"UNKNOWN\" case a \u003d\u003e a}}\n                                       \n val Cust_Identification_udf\u003dudf(Cust_Identification)\n  val train_data_new12 \u003d train_data_new11.withColumn(\"Cust_Identification\", Cust_Identification_udf(col(\"Cust_Identification\")))\n\ntrain_data_new12.toDF.registerTempTable(\"sbank13\")",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Cust_Identification: String \u003d\u003e String \u003d \u003cfunction1\u003e\nCust_Identification_udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\n\u003cconsole\u003e:77: error: not found: value train_data_new11\n         val train_data_new12 \u003d train_data_new11.withColumn(\"Cust_Identification\", Cust_Identification_udf(col(\"Cust_Identification\")))\n                                ^\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523139842235_-1829721437",
      "id": "20180402-173832_1596700718",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "14. Remove columns  tipodom [Address_Type],cod_prov [Province_Code],conyuemp [Spouse_Index],   ult_fec_cli_lt [Last_Date_Primary_Cust]",
      "text": "val train_data_new14 \u003d train_data_new13.drop(\"Address_Type\",\"Province_Code\",\"Spouse_Index\",\"Last_Date_Primary_Cust\")\n",
      "dateUpdated": "Apr 7, 2018 6:24:02 PM",
      "config": {
        "editorSetting": {
          "language": "scala"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1523139842235_-1829721437",
      "id": "20180402-173928_775081766",
      "dateCreated": "Apr 7, 2018 6:24:02 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Data-Cleaning-1",
  "id": "2DDC4VAEM",
  "angularObjects": {
    "2DAVGMF7R:shared_process": [],
    "2DBVU2576:shared_process": [],
    "2D8STWDCM:shared_process": [],
    "2DAVA43S2:shared_process": [],
    "2DBKXU5BJ:shared_process": [],
    "2D89B3Z5Z:shared_process": [],
    "2D8ZA7H9R:shared_process": [],
    "2D8B793QR:shared_process": [],
    "2DBKWSBPS:shared_process": [],
    "2D8ZET3ZD:shared_process": [],
    "2D8Y3SWSJ:shared_process": [],
    "2D88K6WCM:shared_process": [],
    "2D893DVM7:shared_process": [],
    "2DAZEX8F8:shared_process": [],
    "2DATV9FU6:shared_process": [],
    "2D9RA1B4V:shared_process": [],
    "2D9S7NZ95:shared_process": [],
    "2DAYQP1W5:shared_process": [],
    "2DC2X879S:shared_process": []
  },
  "config": {},
  "info": {}
}